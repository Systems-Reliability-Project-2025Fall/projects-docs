\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hidelinks,colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Project Proposal: AIOpsLab Evaluation}
\author{\IEEEauthorblockN{Kosumi Chan}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{The University of North Carolina at
Chapel Hill}\\
kosumi@cs.unc.edu
% City, Country \\
% }
\and
\IEEEauthorblockN{Chaitanya}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{The University of North Carolina at
Chapel Hill}\\
%City, Country \\
chaitany@cs.unc.edu}

\maketitle

% \begin{abstract}
% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
% or Math in Paper Title or Abstract.
% \end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{Introduction}

Modern IT applications increasingly rely on large-scale, distributed architectures such as microservices and serverless computing. While these designs enable scalability and agility, they also introduce significant operational complexity. Failures are no longer isolated; instead, they cascade across dependent services, amplifying the blast radius of incidents. Even a short outage in hyperscale cloud platforms can result in losses of millions of dollars per hour, underscoring the critical importance of rapid detection, diagnosis, and recovery.

Traditionally, incident response has relied heavily on human operators such as Site Reliability Engineers (SREs) who iteratively observe telemetry, localize root causes, and execute mitigation actions. However, the growing scale and complexity of modern systems render manual operations increasingly slow, error-prone, and unsustainable. This motivates a paradigm shift toward \textit{AIOps}---the application of artificial intelligence to automate IT operations. Recent progress in large language models (LLMs) and agent-based architectures has pushed this vision further, enabling autonomous systems that can reason over telemetry, interact with cloud environments, and carry out multi-step incident management tasks.

The long-term goal is to realize \textit{autonomous self-healing clouds}, where intelligent agents manage incidents end-to-end with minimal human involvement. Achieving this requires systematic evaluation of AIOps agents under realistic and diverse operational conditions. In this project, we focus on \textsc{AIOps}Lab, a holistic framework designed to benchmark and advance AI agents for cloud operations. By reproducing parts of its benchmark, evaluating state-of-the-art models, and extending its fault scenarios, we aim to gain deeper insights into the capabilities and limitations of current AIOps solutions and chart a path toward more robust autonomous operations.

\section{Background}

\subsection{Cloud, Microservices And Kubernetes}
Cloud-native services increasingly adopt microservices packaged as containers and orchestrated for reliability and scale. Kubernetes (K8s) provides declarative control and reconciliation—Pods run application containers; ReplicaSets and Deployments manage replica count and rolling updates; Services expose stable endpoints—while the control plane (etcd, API server, scheduler, controller manager) coordinates cluster state; kubelet and kube-proxy run per node. Labels/annotations bind resources and drive routing/selection across components. 

K8s resilience comes from stateless components, leader election, backoffs, and level-triggered reconciliation of desired vs.\ observed state; however, this design centralizes state consistency in etcd and the API server. In practice, teams pair this with rich observability—metrics, traces, and logs—to operate distributed services at scale.

\subsection{Faults and Incidents}
Operational incidents typically progress through four stages: \emph{detection}, \emph{triage}, \emph{diagnosis/root cause analysis}, and \emph{mitigation}. Evaluations commonly track task success and time-based metrics such as Time-to-Detect (TTD) and Time-to-Mitigate (TTM) \cite{chen2025aiopslab}. 

Real-world and injected-fault studies \cite{10646977} show K8s failures cluster into timing delays, resource mis-allocation (less/more than intended), service networking errors, cluster stalls, and cluster-wide outages. Notably, even a \emph{single} bad state value can propagate to overloads or outages; in one large campaign: $\sim$3\% cluster-wide failures, $\sim$24\% under/over-provisioning, and $\sim$4\% service-network issues were observed. Dependency/label errors accounted for 51\% of critical failures, and commonplace misconfigurations readily saturated resources; many errors escaped monitoring until they manifested as failures—underscoring the need to log/guard label changes and to instrument alerts that track convergence of desired vs.\ observed state.


\subsection{Compound AI Systems(AI agents) and AIOps}
Ideally, given a clear problem description, an LLM should be able to return an answer in one step, as is the case for numerous simple tasks. However, many real-world tasks require multi-step reasoning for 2 reasons: 
        \begin{itemize}
                \item The initial context is usually insufficient for tackling the task so the LLM needs to get more context information in the following steps  
                \item Even if a complete context is available, a longer context increases the risk of hallucination and failure, and there is a limit on context length.
        \end{itemize}

        CloudOps is such a real-world task with insufficient initial context. Cloud faults are unpredictable in general, can be non-deterministic or latent. Human DevOps engineers or SREs(Site Reliability Engineer) detect, localize and mitigate cloud faults by observing and acting iteratively with tools. 

        AIOps automates human reasoning with LLM reasoning, but the agent still carries out the task by observing and acting with tools in multiple steps.     

        State-of-Art LLM agent architectures include ReAct\cite{yao2023reactsynergizingreasoningacting} and FLASH\cite{zhang2024flash}.

\subsection{\textsc{AIOps}LAB}
\textsc{AIOps}LAB\cite{chen2025aiopslab} is an comprehensive framework for evaluation of AIOps agents in realistic microservice environments.  

AIOpsLab integrates multiple components, including a workload generator, extensible fault library, telemetry collectors, and an orchestrator that provides a unified \textbf{Agent-Cloud Interface (ACI)}. This design enables agents to interact dynamically with cloud environments, perform tasks such as detection, localization, root-cause analysis, and mitigation, and receive structured feedback. By offering benchmark problems across diverse operational scenarios, AIOpsLab serves as a testbed for evaluating state-of-the-art LLM-based agents.

\section{Project tasks}
\subsection{Partial Reproduction of AIOpsLab benchmark}
Ownership: Kangcheng Chen

This part aims to reproduce part of table 4(agent performance by task) in the AIOPsLab paper. We plan to do this for 3 agents: GPT-4-W-SHELL, ReAct and FLASH and for at least 2 tasks.

This step ensures that our setup of the AIOpsLab aligns well with the setup in the paper.

Estimated timeline: finish before October. 
\subsection{Evaluate State-of-Art models and the impact of thinking budget}
Ownership: Chaitanya

We will benchmark closed-source models (ChatGPT~5, Gemini~2.5~Pro) and open-source models if time permits (DeepSeek-R1 or LLaMA~4) as agents across the incident lifecycle—\emph{detection, localization, root-cause analysis (RCA), mitigation}.

\textbf{Thinking budget (step/compute budget).} We will parameterize each agent’s maximum decision horizon $K$ (allowed action/observation steps) and per-step token ceiling, and study accuracy–latency–cost tradeoffs. Prior evidence shows performance can improve with more steps for some agents, plateauing near $K\!\approx\!15$–$20$; for other models, larger $K$ mainly increases tokens without accuracy gains—motivating careful budget tuning.

Estimated timeline: finish before October. 
\subsection{Construct new faults that are closer to real-world settings}
Ownership: both 

AIOpsLab has a set of simplified, isolated faults but how does an agent perform facing real-world systemic faults? We plan to introduce new faults to AIOPsLab inspired by production incidents and evaluate agent performance on them. If time permits, we may also add a new cloud application to the benchmark.

Estimated timeline: finish before November. 


\bibliography{refs}
\bibliographystyle{plain}

\vspace{12pt}

\end{document}
